{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0fca92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import evaluate\n",
    "\n",
    "\n",
    "\n",
    "from fintuned_use_parameter import fintuned_use_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01757015",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = fintuned_use_parameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33569218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_predict(text, tokenizer_, model_, label_list):\n",
    "    #print(text)\n",
    "    t_tokenize = tokenizer_(text, max_length=parameter.max_length, truncation=True, padding=\"max_length\", return_tensors='pt').to(parameter.device)\n",
    "    with torch.no_grad():\n",
    "        output = model_(**t_tokenize).logits\n",
    "        #print(output)\n",
    "        preds = torch.where(output > 0, 1, 0)\n",
    "    label = [[label_list[idx] for idx in range(len(pred)) if pred[idx] == 1] for pred in preds][0]\n",
    "    label = '|'.join(label)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff4d64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "file-name:  ./data/test/sokushin_test.xlsx\n",
      "model-name:  ./result/sokushin\n",
      "tokenizer-name:  nlp-waseda/roberta-base-japanese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wakasugi/anaconda3/envs/transformers2023/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n"
     ]
    }
   ],
   "source": [
    "for file in parameter.use_list:\n",
    "    file = os.path.join(parameter.use_data_dir, file)\n",
    "    print(\"==========\")\n",
    "    print('file-name: ', file)\n",
    "    try:\n",
    "        file_pd = pd.read_excel(file)\n",
    "    except FileNotFoundError:\n",
    "        file_pd = pd.read_csv(file, encoding='cp932')\n",
    "    for idx, (label_column_name, label_list) in enumerate(zip(parameter.label_column_name, parameter.label_list)):\n",
    "        print('model-name: ', parameter.use_model_dir_list[idx])\n",
    "        print('tokenizer-name: ',parameter.use_tokenizer[idx])\n",
    "        tokenizer = AutoTokenizer.from_pretrained(parameter.use_tokenizer[idx])\n",
    "        #ここのモデルは自前の場合は手作業で変更\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(parameter.use_model_dir_list[idx], num_labels=len(label_list),problem_type = \"multi_label_classification\").to(parameter.device)\n",
    "        file_pd[label_column_name] = file_pd[parameter.text_column_name].apply(\n",
    "            lambda x: multi_predict(str(x), \n",
    "                                    tokenizer_ = tokenizer, \n",
    "                                    model_= model,\n",
    "                                    label_list = label_list)\n",
    "            )\n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78922a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ツイート</th>\n",
       "      <th>判定結果</th>\n",
       "      <th>促進軸</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>残念 旦那 の 実家 が 九州 南部 で 台風 回 バードストライク 回 と 滞在 が 延び...</td>\n",
       "      <td>その他</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>さっき の 東京 の 大雨 で 飯田橋 の 神田川 が 氾濫 危険 水位 途 に 階梯 浸水...</td>\n",
       "      <td>推奨|行動抑制</td>\n",
       "      <td>推奨|行動抑制</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>です ね 使い 切って から 購入 し ましょ 私 は 乾燥 肌 敏感 肌 です が スキン...</td>\n",
       "      <td>推奨</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>おはよう 御座います 台風 の 進路 に 注意 し つつ 笑顔 で 迎える 良い 週 末 を</td>\n",
       "      <td>推奨</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>千葉 の 台風 で ください した 方々 へ 送って 被災</td>\n",
       "      <td>願望</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>本日 ずーっと 晴天 気仙沼 日和 その 台風 一 過 の 割に 凄く 綺麗な 海 でした ...</td>\n",
       "      <td>推奨</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>たぶん ピーク 過ぎ たわ なかなか 元気 の ある 台風 や から そっち 行ったら みん...</td>\n",
       "      <td>推奨</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>ピオ たん お は ピオニュース を 見て ビックリ して る よ 生き物 を 扱う お 仕...</td>\n",
       "      <td>その他</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>土砂 災害 に 警戒 を 大型で 非常に 強い 台風 号 の 影響 で 宮崎 県 で は 土...</td>\n",
       "      <td>推奨</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>おはよう ございます ニャ 猫 風 強まり 雨 も 徐々に 降り 始めて 来 ました ハウス...</td>\n",
       "      <td>願望</td>\n",
       "      <td>願望</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3243 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ツイート     判定結果      促進軸\n",
       "0     残念 旦那 の 実家 が 九州 南部 で 台風 回 バードストライク 回 と 滞在 が 延び...      その他       推奨\n",
       "1     さっき の 東京 の 大雨 で 飯田橋 の 神田川 が 氾濫 危険 水位 途 に 階梯 浸水...  推奨|行動抑制  推奨|行動抑制\n",
       "2     です ね 使い 切って から 購入 し ましょ 私 は 乾燥 肌 敏感 肌 です が スキン...       推奨       推奨\n",
       "3        おはよう 御座います 台風 の 進路 に 注意 し つつ 笑顔 で 迎える 良い 週 末 を       推奨       推奨\n",
       "4                         千葉 の 台風 で ください した 方々 へ 送って 被災       願望         \n",
       "...                                                 ...      ...      ...\n",
       "3238  本日 ずーっと 晴天 気仙沼 日和 その 台風 一 過 の 割に 凄く 綺麗な 海 でした ...       推奨         \n",
       "3239  たぶん ピーク 過ぎ たわ なかなか 元気 の ある 台風 や から そっち 行ったら みん...       推奨       推奨\n",
       "3240  ピオ たん お は ピオニュース を 見て ビックリ して る よ 生き物 を 扱う お 仕...      その他       推奨\n",
       "3241  土砂 災害 に 警戒 を 大型で 非常に 強い 台風 号 の 影響 で 宮崎 県 で は 土...       推奨       推奨\n",
       "3242  おはよう ございます ニャ 猫 風 強まり 雨 も 徐々に 降り 始めて 来 ました ハウス...       願望       願望\n",
       "\n",
       "[3243 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db562088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a1770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
