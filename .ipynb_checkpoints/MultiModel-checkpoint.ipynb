{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e38afff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import evaluate\n",
    "\n",
    "\n",
    "\n",
    "from InitialParameter import InitialParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21f18f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = InitialParameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56d79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74eb3c24",
   "metadata": {},
   "source": [
    "# Dataset作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58cb018e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wakasugi/anaconda3/envs/transformers2023/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at nlp-waseda/roberta-base-japanese were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nlp-waseda/roberta-base-japanese and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#データ読み込み\n",
    "for trafile,tesfile, use_tokenize in zip(parameter.train_list, parameter.test_list, parameter.use_model_list):\n",
    "    train_pd = pd.read_excel(os.path.join(parameter.train_data_dir, trafile))\n",
    "    train_dataset = Dataset.from_pandas(train_pd)\n",
    "    \n",
    "    test_pd = pd.read_excel(os.path.join(parameter.test_data_dir, tesfile))\n",
    "    test_dataset = Dataset.from_pandas(test_pd)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(use_tokenize)\n",
    "    model = AutoModel.from_pretrained(use_tokenize)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bca8ae62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ツイート</th>\n",
       "      <th>判定結果</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今日 も 一 日 お 疲れ 様 でした 台風 の 影響 で 離れて る 地域 でも 雨 風 ...</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>なんか テレビ の 他人事 感 凄い な とりあえず 南房総 報道 し ときゃ いい んでし...</td>\n",
       "      <td>願望</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>皆さん 台風 大丈夫 で しか 初めて お家 の 電気 が 少し 消えて びっくり し まし...</td>\n",
       "      <td>励まし</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>今回 に 台風 は 本当の 悪い 子 が 人 いろんな 迷惑 がって る 反省 して ほしい</td>\n",
       "      <td>願望</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>台風 号 被害 を られた 方々 に 心 より お 見舞い 申し上げ ます さん で は を...</td>\n",
       "      <td>願望</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12966</th>\n",
       "      <td>台風 台風 な のです 低 気圧 が 気 を 付けて ください ませ</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12967</th>\n",
       "      <td>通電 火災 知って る 去年 の 台風 の 時 一応 電気 屋 さんや から 会社 の 人 ...</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12968</th>\n",
       "      <td>お は シグ ねー ⊃ アワアワ 今日 一体 育祭 ので 休み です が 吹奏 楽部 に 休...</td>\n",
       "      <td>推奨|励まし</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12969</th>\n",
       "      <td>台風 も 来て ます しお気 を つけて お 帰り 下さい ませ ゚ Д ゚</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12970</th>\n",
       "      <td>今日 は 祝日 素敵な お 休み を 満喫 する に は 座 で ショー を 楽しみ お 酒...</td>\n",
       "      <td>推奨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12971 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ツイート    判定結果\n",
       "0      今日 も 一 日 お 疲れ 様 でした 台風 の 影響 で 離れて る 地域 でも 雨 風 ...      推奨\n",
       "1      なんか テレビ の 他人事 感 凄い な とりあえず 南房総 報道 し ときゃ いい んでし...      願望\n",
       "2      皆さん 台風 大丈夫 で しか 初めて お家 の 電気 が 少し 消えて びっくり し まし...     励まし\n",
       "3         今回 に 台風 は 本当の 悪い 子 が 人 いろんな 迷惑 がって る 反省 して ほしい      願望\n",
       "4      台風 号 被害 を られた 方々 に 心 より お 見舞い 申し上げ ます さん で は を...      願望\n",
       "...                                                  ...     ...\n",
       "12966                 台風 台風 な のです 低 気圧 が 気 を 付けて ください ませ      推奨\n",
       "12967  通電 火災 知って る 去年 の 台風 の 時 一応 電気 屋 さんや から 会社 の 人 ...      推奨\n",
       "12968  お は シグ ねー ⊃ アワアワ 今日 一体 育祭 ので 休み です が 吹奏 楽部 に 休...  推奨|励まし\n",
       "12969             台風 も 来て ます しお気 を つけて お 帰り 下さい ませ ゚ Д ゚      推奨\n",
       "12970  今日 は 祝日 素敵な お 休み を 満喫 する に は 座 で ショー を 楽しみ お 酒...      推奨\n",
       "\n",
       "[12971 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fb0b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "868160af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3243 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ツイート': '今日 も 日 お疲れさま そちら 東京 は 台風 が 近付いて きて い ます ね 明日 から 明後日 に かけて 影響 が 出て くる の か な くれぐれも お 気 を つけて ください な ラベル を 取って から 飲み ます よ 萌花 さん 鈴木 萌花 さん 流 ーの 飲み 方 次回 遠征 拝見', '判定結果': '推奨', 'input_ids': [2, 2278, 284, 286, 431, 31743, 1104, 9181, 18720, 364, 391, 265, 4135, 268, 28821, 1602, 3570, 398, 533, 1626, 9240, 285, 1731, 3252, 2177, 263, 733, 635, 268, 1786, 1814, 261, 343, 528, 6496, 23963, 1717, 431, 1559, 266, 5068, 3596, 528, 15761, 266, 5240, 285, 7628, 533, 1303, 18466, 2852, 1148, 2911, 18466, 2852, 1148, 1490, 275, 638, 353, 7628, 647, 12574, 3861, 9435, 1575, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1.0, 0.0, 0.0, 0.0]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ツイート', '判定結果', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12971\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ツイート', '判定結果', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3243\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def processing(example):\n",
    "    tokenized = tokenizer(example[parameter.text_column_name], max_length=parameter.max_length, truncation=True, padding=\"max_length\")\n",
    "    judges = example[parameter.label_column_name] #判定結果を取り出す\n",
    "    #judges = judges.split('|')\n",
    "\n",
    "    labels = []\n",
    "    for idx in range(len(judges)):\n",
    "        dummy_label = [] #ラベルのダミー列を取得\n",
    "        judge = judges[idx].split('|')\n",
    "        for label in parameter.label_list:\n",
    "            if label in judge:\n",
    "                dummy_label.append(1.0)\n",
    "            else:\n",
    "                dummy_label.append(0.0)\n",
    "        labels.append(dummy_label)\n",
    "    tokenized['labels'] = labels\n",
    "        \n",
    "        #tokenized = {k: torch.tensor(v).to(parameter.device) for k, v in tokenized.items()}\n",
    "    return tokenized\n",
    "dataset_tokenized = dataset.map(processing,batched=True)\n",
    "print(dataset_tokenized['train'][1000])\n",
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c4c04e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_tokenized = dataset_tokenized.remove_columns([parameter.text_column_name, parameter.label_column_name])\n",
    "\n",
    "dataset_tokenized.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd858eb",
   "metadata": {},
   "source": [
    "# モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be29164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n",
    "    def __init__(self, model, num_labels):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.linear = torch.nn.linear(\n",
    "            self.model.config.hidden_size, num_labels\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        token_type_ids = None,\n",
    "        labels = None,\n",
    "    ):\n",
    "        model_ouptut = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        last_hidden_state = model_ouptut.last_hidden_state\n",
    "        \n",
    "        average_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
    "        \n",
    "        scores = self.linear(average_hidden_state)\n",
    "        \n",
    "        output = {'logits': scores}\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n",
    "            output['loss'] = loss\n",
    "            output['label_ids'] = labels\n",
    "        output['predictions'] = average_hidden_state\n",
    "        output = type('prediction',(object,),output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35af0947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlp-waseda/roberta-base-japanese were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at nlp-waseda/roberta-base-japanese and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "modelc = AutoModelForSequenceClassification.from_pretrained(use_tokenize,num_labels=len(parameter.label_list),problem_type = \"multi_label_classification\").to(parameter.device)\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1075a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    probability = pred.predictions\n",
    "    preds = np.where(pred.predictions > 0, 1, 0)\n",
    "    f1 = f1_score(labels, preds,average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds,average=\"macro\")\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "    \n",
    "    \n",
    "    for idx, label_ in enumerate(parameter.label_list):\n",
    "        fpr, tpr, thresholds = roc_curve(labels[:, idx], probability[:, idx])\n",
    "        auc = roc_auc_score(labels,probability,average=\"macro\")\n",
    "        plt.plot(fpr, tpr, label=f'class: {label_}')\n",
    "        print(f'auc_{label_},== {roc_auc_score(labels[:, idx], probability[:, idx])}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(labels,probability,average=\"macro\")\n",
    "    return {\"accuracy\":acc,\"precision\":precision,'recall': recall, \"f1\":f1,\n",
    "            'auc':auc}\n",
    "\n",
    "#citeList = ['test_loss', 'test_accuracy']\n",
    "def figure_show(train_metrics, val_metrics,save_dir=None, save_name='fig', citeList=['test_loss', 'test_accuracy']):\n",
    "    fig, axes = plt.subplots(len(citeList),1,figsize=(8,10))\n",
    "    x = [i for i in range(1,N_split+1)]\n",
    "\n",
    "    for cite,ax in zip(citeList,axes):\n",
    "        tem_tra = []\n",
    "        tem_val = []\n",
    "        for tra,val in zip(train_metrics,val_metrics):\n",
    "            tem_tra.append(tra[cite])\n",
    "            tem_val.append(val[cite])\n",
    "        ax.plot(x,tem_val,label=\"Validation\")\n",
    "        ax.plot(x,tem_tra,label=\"Train\")\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(cite.split('_')[1])\n",
    "        ax.set_title(cite.split('_')[1])\n",
    "        ax.legend(loc=2)\n",
    "        ax.grid()\n",
    "    fig.align_labels()\n",
    "    plt.show()\n",
    "    #fig.savefig(f\"{save_dir}/{os.path.split(save_file)[-1]}.png\", format=\"png\")\n",
    "    if save_dir is not None:\n",
    "        fig.savefig(f\"{save_dir}/fig_{save_name}.png\", format=\"png\")\n",
    "        #fig.savefig(f\"{save_dir}/fig_{os.path.split(save_dir)[-1]}.png\", format=\"png\")\n",
    "    else:\n",
    "        print('NONE')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f18a8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = np.array([{k:v.to(parameter.device) for k, v in data.items()} for data in dataset_tokenized['train']])\n",
    "dataset_test = np.array([{k:v.to(parameter.device) for k, v in data.items()} for data in dataset_tokenized['test']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d892a9",
   "metadata": {},
   "source": [
    "# 学習開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "71a34d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10376\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'aall_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [119], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      9\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodelc,\n\u001b[1;32m     10\u001b[0m         data_collator \u001b[38;5;241m=\u001b[39m data_collator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#trainer.train()\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     train_metrics\u001b[38;5;241m.\u001b[39mappend(trainer\u001b[38;5;241m.\u001b[39mpredict(\u001b[43maall_data\u001b[49m[train])\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m     18\u001b[0m     val_metrics\u001b[38;5;241m.\u001b[39mappend(trainer\u001b[38;5;241m.\u001b[39mpredict(aall_data[test])\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aall_data' is not defined"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "parameter.training_args.num_train_epochs = 1\n",
    "\n",
    "train_metrics = []\n",
    "val_metrics = []\n",
    "for train, test in kf.split(dataset_train):\n",
    "    print(len(train))\n",
    "    trainer = Trainer(\n",
    "        model=modelc,\n",
    "        data_collator = data_collator,\n",
    "        args=parameter.training_args,\n",
    "        train_dataset=dataset_train[train],\n",
    "        eval_dataset=dataset_train[test],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    #trainer.train()\n",
    "    train_metrics.append(trainer.predict(dataset_train[train]).metrics)\n",
    "    val_metrics.append(trainer.predict(dataset_train[test]).metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bf6be31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 3243\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_推奨,== 0.9620168734389853\n",
      "auc_行動抑制,== 0.9982382813031818\n",
      "auc_励まし,== 0.9755640255640255\n",
      "auc_願望,== 0.9904408857255832\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 2.5067115, -6.9330316, -3.2360642, -4.095049 ],\n",
       "       [ 4.1602173,  1.9419556, -5.110614 , -5.278173 ],\n",
       "       [ 3.813549 , -5.68594  , -5.4019327, -5.3943086],\n",
       "       ...,\n",
       "       [ 1.5673428, -7.3178535, -4.9434233, -1.889775 ],\n",
       "       [ 5.6843295, -3.920227 , -5.3974133, -5.0942254],\n",
       "       [-1.9883912, -5.8534737, -5.075929 ,  3.28599  ]], dtype=float32), label_ids=array([[0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32), metrics={'test_loss': 0.12387678027153015, 'test_accuracy': 0.8418131359851989, 'test_precision': 0.9272146962877206, 'test_recall': 0.8897795321257707, 'test_f1': 0.9074122671625189, 'test_auc': 0.9815650165079439, 'test_runtime': 11.2965, 'test_samples_per_second': 287.08, 'test_steps_per_second': 17.97})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c9ad03fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/wakasugi/model_create/MultiFacilitatedAxis/result/sokushin\n",
      "Configuration saved in /home/wakasugi/model_create/MultiFacilitatedAxis/result/sokushin/config.json\n",
      "Model weights saved in /home/wakasugi/model_create/MultiFacilitatedAxis/result/sokushin/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(os.path.join(parameter.save_dir, 'sokushin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4325a",
   "metadata": {},
   "source": [
    "# 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "89536394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /home/wakasugi/.cache/huggingface/hub/models--nlp-waseda--roberta-base-japanese/snapshots/49ce73ecd6eb4dab3c1c06edaaab35892b3cea80/spiece.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/wakasugi/.cache/huggingface/hub/models--nlp-waseda--roberta-base-japanese/snapshots/49ce73ecd6eb4dab3c1c06edaaab35892b3cea80/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/wakasugi/.cache/huggingface/hub/models--nlp-waseda--roberta-base-japanese/snapshots/49ce73ecd6eb4dab3c1c06edaaab35892b3cea80/tokenizer_config.json\n",
      "/home/wakasugi/anaconda3/envs/transformers2023/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "loading configuration file /home/wakasugi/model_create/MultiFacilitatedAxis/result/sokushin/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/home/wakasugi/model_create/MultiFacilitatedAxis/result/sokushin\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /home/wakasugi/model_create/MultiFacilitatedAxis/result/sokushin/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /home/wakasugi/model_create/MultiFacilitatedAxis/result/sokushin.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenzer = AutoTokenizer.from_pretrained(parameter.use_model_list[0])\n",
    "modelc = AutoModelForSequenceClassification.from_pretrained(os.path.join(parameter.save_dir, 'sokushin')).to(parameter.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d684a1eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['推奨', '行動抑制']\n",
      "['推奨', '願望']\n",
      "[]\n",
      "['推奨', '励まし']\n",
      "['行動抑制']\n"
     ]
    }
   ],
   "source": [
    "def inference(texts:list[str], tokenizer_=tokenizer, model_=modelc):\n",
    "    for text in texts:\n",
    "        t_tokenize = tokenizer_(text, max_length=parameter.max_length, truncation=True, padding=\"max_length\", return_tensors='pt').to(parameter.device)\n",
    "        #print(t_tokenize)\n",
    "        with torch.no_grad():\n",
    "            output = model_(**t_tokenize).logits\n",
    "        #print(output)\n",
    "        preds = torch.where(output > 0, 1, 0)\n",
    "        #print(preds)\n",
    "        print([[parameter.label_list[idx] for idx in range(len(pred)) if pred[idx] == 1] for pred in preds][0])\n",
    "inference(['前線と台風のセットは大雨になりやすいパターン。冠水しやすい道路、土がむき出しの斜面、川や海・池など水辺は避けて、マンホールや側溝にも気をつけて。竜巻や雷に備えた行動も思い出して下さいね。',\n",
    "          'なっちゃん まさか 颶風 号 が こんなに 影響 及ぼす と は ねー お ウォーター と か 諸々 買物 お 疲れ 様 でした この後 バス 気 を つけて 帰国 てね 早く 住 家 の 方 も 復旧 し 落ち着き ます ように 人',\n",
    "          '台風 怖い けど お ハム さん 達 みたいに みんな と 一緒に いたら 安心です ね',\n",
    "          'おはよう ございます よう の ログイン 画面 連打 ちゃ って スクショ でき なかった 笑 秋 もう です ね 台風 も 去って ちょっと 肌寒い かも しれ ませ ん ね し 風邪 ひか ない プロセカ に 気 を つけて ね 今日 も 一 日 頑張り ましょ ́ ノプロセカ',\n",
    "           '市民 協働 課 防災 担当 です 令和 月日 時分 台風 号 接近 に 伴い 大阪 市 大雨 暴風 が 発表 さ れ まし 今後 も 風 や 雨 が 強まり 落雷 する こと も あり ます ので 不要な 外出 は 控える よう お 願い し ます 協働 課',\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516bc152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc854f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3eca6d7",
   "metadata": {},
   "source": [
    "PredictionOutput(predictions=array([[ 0.04170038, -3.7668915 , -1.486596  , -1.248169  ],\n",
    "       [ 1.3677144 ,  0.720794  , -3.1399653 , -2.5168984 ],\n",
    "       [ 2.0110874 , -2.7480996 , -3.1586976 , -2.693866  ],\n",
    "       ...,\n",
    "       [-0.74171925, -2.93097   , -3.106825  ,  1.5284041 ],\n",
    "       [ 2.6738646 , -1.0764515 , -3.4657722 , -2.9998314 ],\n",
    "       [ 0.18142688, -3.1702743 , -2.9269063 , -1.1237792 ]],\n",
    "      dtype=float32), label_ids=array([[0., 0., 0., 0.],\n",
    "       [1., 1., 0., 0.],\n",
    "       [1., 0., 0., 0.],\n",
    "       ...,\n",
    "       [0., 0., 0., 0.],\n",
    "       [1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1.]], dtype=float32), metrics={'test_loss': 0.22486577928066254, 'test_runtime': 22.0246, 'test_samples_per_second': 147.245, 'test_steps_per_second': 9.217})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c961b79",
   "metadata": {},
   "source": [
    "# 閾値決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f4083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],y_score[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830797f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ee122ef",
   "metadata": {},
   "source": [
    "# メモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6f85f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/wakasugi/.cache/huggingface/hub/models--nlp-waseda--roberta-base-japanese/snapshots/49ce73ecd6eb4dab3c1c06edaaab35892b3cea80/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"nlp-waseda/roberta-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/wakasugi/.cache/huggingface/hub/models--nlp-waseda--roberta-base-japanese/snapshots/49ce73ecd6eb4dab3c1c06edaaab35892b3cea80/pytorch_model.bin\n",
      "Some weights of the model checkpoint at nlp-waseda/roberta-base-japanese were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at nlp-waseda/roberta-base-japanese and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,   294, 27128, 16414, 15843,  3228,  4135,   268,  1829,   263,\n",
      "           284, 22415,   311,   275,   341,   285,  1559,   266,  5068,  1626,\n",
      "          1860,   286,  5357,   269,  2369, 13850,   276, 19384, 13990,  5170,\n",
      "          3008, 11799,  7053,   769,   284,  3259,  1387,  1303, 25607,     3,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'labels': tensor([[1., 0., 0., 0.]], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6838, device='cuda:0'), logits=tensor([[-0.0956, -0.1848,  0.2386, -0.2670]], device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelc = AutoModelForSequenceClassification.from_pretrained(use_tokenize,num_labels=4,problem_type = \"multi_label_classification\").to(parameter.device)\n",
    "tokenized = {k: torch.tensor(v).unsqueeze(0).to(parameter.device) for k, v in dataset_tokenized['train'][10].items()}\n",
    "tokenized['labels'] = tokenized['labels'].float()\n",
    "#train_dataloader = DataLoader(dataset_tokenized['train'], shuffle=True, batch_size=16,collate_fn=coll)\n",
    "print(tokenized)\n",
    "with torch.no_grad():\n",
    "    output = modelc(**tokenized)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb16a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40067d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00410aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
